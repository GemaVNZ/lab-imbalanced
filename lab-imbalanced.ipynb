{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with Credit Card Fraud dataset.\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "- **distance_from_home:** the distance from home where the transaction happened.\n",
    "- **distance_from_last_transaction:** the distance from last transaction happened.\n",
    "- **ratio_to_median_purchase_price:** Ratio of purchased price transaction to median purchase price.\n",
    "- **repeat_retailer:** Is the transaction happened from same retailer.\n",
    "- **used_chip:** Is the transaction through chip (credit card).\n",
    "- **used_pin_number:** Is the transaction happened by using PIN number.\n",
    "- **online_order:** Is the transaction an online order.\n",
    "- **fraud:** Is the transaction fraudulent. **0=legit** -  **1=fraud**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.877857</td>\n",
       "      <td>0.311140</td>\n",
       "      <td>1.945940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.829943</td>\n",
       "      <td>0.175592</td>\n",
       "      <td>1.294219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.091079</td>\n",
       "      <td>0.805153</td>\n",
       "      <td>0.427715</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.247564</td>\n",
       "      <td>5.600044</td>\n",
       "      <td>0.362663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.190936</td>\n",
       "      <td>0.566486</td>\n",
       "      <td>2.222767</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home  distance_from_last_transaction  \\\n",
       "0           57.877857                        0.311140   \n",
       "1           10.829943                        0.175592   \n",
       "2            5.091079                        0.805153   \n",
       "3            2.247564                        5.600044   \n",
       "4           44.190936                        0.566486   \n",
       "\n",
       "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
       "0                        1.945940              1.0        1.0   \n",
       "1                        1.294219              1.0        0.0   \n",
       "2                        0.427715              1.0        0.0   \n",
       "3                        0.362663              1.0        1.0   \n",
       "4                        2.222767              1.0        1.0   \n",
       "\n",
       "   used_pin_number  online_order  fraud  \n",
       "0              0.0           0.0    0.0  \n",
       "1              0.0           0.0    0.0  \n",
       "2              0.0           1.0    0.0  \n",
       "3              0.0           1.0    0.0  \n",
       "4              0.0           1.0    0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/card_transdata.csv\")\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1.** What is the distribution of our target variable? Can we say we're dealing with an imbalanced dataset?\n",
    "- **2.** Train a LogisticRegression.\n",
    "- **3.** Evaluate your model. Take in consideration class importance, and evaluate it by selection the correct metric.\n",
    "- **4.** Run **Oversample** in order to balance our target variable and repeat the steps above, now with balanced data. Does it improve the performance of our model? \n",
    "- **5.** Now, run **Undersample** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model?\n",
    "- **6.** Finally, run **SMOTE** in order to balance our target variable and repeat the steps above (1-3), now with balanced data. Does it improve the performance of our model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   distance_from_home  distance_from_last_transaction  \\\n",
      "0           57.877857                        0.311140   \n",
      "1           10.829943                        0.175592   \n",
      "2            5.091079                        0.805153   \n",
      "3            2.247564                        5.600044   \n",
      "4           44.190936                        0.566486   \n",
      "\n",
      "   ratio_to_median_purchase_price  repeat_retailer  used_chip  \\\n",
      "0                        1.945940              1.0        1.0   \n",
      "1                        1.294219              1.0        0.0   \n",
      "2                        0.427715              1.0        0.0   \n",
      "3                        0.362663              1.0        1.0   \n",
      "4                        2.222767              1.0        1.0   \n",
      "\n",
      "   used_pin_number  online_order  fraud  \n",
      "0              0.0           0.0    0.0  \n",
      "1              0.0           0.0    0.0  \n",
      "2              0.0           1.0    0.0  \n",
      "3              0.0           1.0    0.0  \n",
      "4              0.0           1.0    0.0  \n",
      "fraud\n",
      "0.0    0.912597\n",
      "1.0    0.087403\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(fraud[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraud\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))  \u001b[38;5;66;03m# Distribution of target variable\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the distribution of the target variable\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m sns\u001b[38;5;241m.\u001b[39mcountplot(fraud[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfraud\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDistribution of Fraud (Target Variable)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:2675\u001b[0m, in \u001b[0;36mcountplot\u001b[1;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, hue_norm, stat, width, dodge, gap, log_scale, native_scale, formatter, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2671\u001b[0m     p\u001b[38;5;241m.\u001b[39mplot_data[count_axis] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(p\u001b[38;5;241m.\u001b[39mplot_data) \u001b[38;5;241m/\u001b[39m denom\n\u001b[0;32m   2673\u001b[0m aggregator \u001b[38;5;241m=\u001b[39m EstimateAggregator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, errorbar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2675\u001b[0m p\u001b[38;5;241m.\u001b[39mplot_bars(\n\u001b[0;32m   2676\u001b[0m     aggregator\u001b[38;5;241m=\u001b[39maggregator,\n\u001b[0;32m   2677\u001b[0m     dodge\u001b[38;5;241m=\u001b[39mdodge,\n\u001b[0;32m   2678\u001b[0m     width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[0;32m   2679\u001b[0m     gap\u001b[38;5;241m=\u001b[39mgap,\n\u001b[0;32m   2680\u001b[0m     color\u001b[38;5;241m=\u001b[39mcolor,\n\u001b[0;32m   2681\u001b[0m     fill\u001b[38;5;241m=\u001b[39mfill,\n\u001b[0;32m   2682\u001b[0m     capsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2683\u001b[0m     err_kws\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m   2684\u001b[0m     plot_kws\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   2685\u001b[0m )\n\u001b[0;32m   2687\u001b[0m p\u001b[38;5;241m.\u001b[39m_add_axis_labels(ax)\n\u001b[0;32m   2688\u001b[0m p\u001b[38;5;241m.\u001b[39m_adjust_cat_axis(ax, axis\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39morient)\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\seaborn\\categorical.py:1280\u001b[0m, in \u001b[0;36m_CategoricalPlotter.plot_bars\u001b[1;34m(self, aggregator, dodge, gap, width, fill, color, capsize, err_kws, plot_kws)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_vars, sub_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_data(iter_vars,\n\u001b[0;32m   1274\u001b[0m                                          from_comp_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1275\u001b[0m                                          allow_empty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1277\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axes(sub_vars)\n\u001b[0;32m   1279\u001b[0m     agg_data \u001b[38;5;241m=\u001b[39m sub_data \u001b[38;5;28;01mif\u001b[39;00m sub_data\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m (\n\u001b[1;32m-> 1280\u001b[0m         sub_data\n\u001b[0;32m   1281\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient)\n\u001b[0;32m   1282\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(aggregator, agg_var, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgroupby_apply_include_groups(\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m   1283\u001b[0m         \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m   1284\u001b[0m     )\n\u001b[0;32m   1286\u001b[0m     agg_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m width \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_native_width\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dodge:\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1816\u001b[0m     f \u001b[38;5;241m=\u001b[39m func\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_groups:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n\u001b[0;32m   1821\u001b[0m \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m   1822\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1852\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1857\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1885\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1887\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:909\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# This calls DataSplitter.__iter__\u001b[39;00m\n\u001b[0;32m    907\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(group_keys, splitter)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, group \u001b[38;5;129;01min\u001b[39;00m zipped:\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# Pinning name is needed for\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;66;03m#  test_group_apply_once_per_group,\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;66;03m#  test_inconsistent_return_type, test_set_group_name,\u001b[39;00m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;66;03m#  test_group_name_available_in_inference_pass,\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m#  test_groupby_multi_timezone\u001b[39;00m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(group, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, key)\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:1160\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1157\u001b[0m starts, ends \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mgenerate_slices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slabels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups)\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(starts, ends):\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chop(sdata, \u001b[38;5;28mslice\u001b[39m(start, end))\n",
      "File \u001b[1;32mc:\\Users\\Gema\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:1186\u001b[0m, in \u001b[0;36mFrameSplitter._chop\u001b[1;34m(self, sdata, slice_obj)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chop\u001b[39m(\u001b[38;5;28mself\u001b[39m, sdata: DataFrame, slice_obj: \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# Fastpath equivalent to:\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m# if self.axis == 0:\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;66;03m#     return sdata.iloc[slice_obj]\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m     \u001b[38;5;66;03m#     return sdata.iloc[:, slice_obj]\u001b[39;00m\n\u001b[1;32m-> 1186\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m sdata\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mget_slice(slice_obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1187\u001b[0m     df \u001b[38;5;241m=\u001b[39m sdata\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(mgr, axes\u001b[38;5;241m=\u001b[39mmgr\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39m__finalize__(sdata, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "print(fraud.head())\n",
    "print(fraud['fraud'].value_counts(normalize=True))  # Distribution of target variable\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "sns.countplot(fraud['fraud'])\n",
    "plt.title('Distribution of Fraud (Target Variable)')\n",
    "plt.show()\n",
    "\n",
    "# Identify imbalance\n",
    "is_imbalanced = fraud['fraud'].value_counts(normalize=True).max() > 0.75\n",
    "print(f\"Is the dataset imbalanced? {'Yes' if is_imbalanced else 'No'}\")\n",
    "\n",
    "# Split the data\n",
    "X = fraud.drop(columns=['fraud'])\n",
    "y = fraud['fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Initial Model Evaluation\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Oversampling using resample\n",
    "print(\"\\nOversampling...\")\n",
    "data_majority = fraud[fraud.fraud == 0]\n",
    "data_minority = fraud[fraud.fraud == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "data_minority_upsampled = resample(data_minority,\n",
    "                                   replace=True,\n",
    "                                   n_samples=len(data_majority),\n",
    "                                   random_state=42)\n",
    "\n",
    "data_oversampled = pd.concat([data_majority, data_minority_upsampled])\n",
    "X_over = data_oversampled.drop(columns=['fraud'])\n",
    "y_over = data_oversampled['fraud']\n",
    "\n",
    "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over, y_over, test_size=0.3, random_state=42, stratify=y_over)\n",
    "model.fit(X_train_over, y_train_over)\n",
    "y_pred_over = model.predict(X_test_over)\n",
    "\n",
    "print(\"Evaluation with Oversampled Data\")\n",
    "print(confusion_matrix(y_test_over, y_pred_over))\n",
    "print(classification_report(y_test_over, y_pred_over))\n",
    "\n",
    "# Undersampling\n",
    "print(\"\\nUndersampling...\")\n",
    "data_majority_downsampled = resample(data_majority,\n",
    "                                     replace=False,\n",
    "                                     n_samples=len(data_minority),\n",
    "                                     random_state=42)\n",
    "\n",
    "data_undersampled = pd.concat([data_majority_downsampled, data_minority])\n",
    "X_under = data_undersampled.drop(columns=['fraud'])\n",
    "y_under = data_undersampled['fraud']\n",
    "\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, test_size=0.3, random_state=42, stratify=y_under)\n",
    "model.fit(X_train_under, y_train_under)\n",
    "y_pred_under = model.predict(X_test_under)\n",
    "\n",
    "print(\"Evaluation with Undersampled Data\")\n",
    "print(confusion_matrix(y_test_under, y_pred_under))\n",
    "print(classification_report(y_test_under, y_pred_under))\n",
    "\n",
    "# SMOTE\n",
    "print(\"\\nSMOTE...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42, stratify=y_smote)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = model.predict(X_test_smote)\n",
    "\n",
    "print(\"Evaluation with SMOTE Data\")\n",
    "print(confusion_matrix(y_test_smote, y_pred_smote))\n",
    "print(classification_report(y_test_smote, y_pred_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
